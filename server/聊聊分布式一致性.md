## 概念

分布式一致性是一个很复杂很复杂很复杂的问题，毕竟它从属于大名鼎鼎的<b>CAP</b>

+ 一致性
+ 可用性
+ 分区容错性

### CAP理论

在讨论后面一致性算法之前，我们必须先了解一下分布式最重要的CAP理论

> Any networked shared-data system can have at most two of the three desired properties.

CAP理论并不是严格的(3选2)理论，而是至多选2理论。

#### 概念

+ C:Consistency(一致性)
+ A:Availability(可用性)
+ P:Partition tolerance(分区容错性)

![image-20200329122025316](/Users/liuyl/Library/Application Support/typora-user-images/image-20200329122025316.png)

我们需要重点聊聊<b>P(分区容错性)</b>。

举个例子:

A和B两个服务在C网络里，此时C网络崩坏(路由器突然无法互通等)，导致C网络被分为C1,C2两个网络，此时A和B分别处于C1，C2网络中。如果AB无法再正常通行，则CA两个特性将无从谈起。

所以P是CAP的前提条件，所谓(3选2理论)其实是(P+2选1理论)

## 算法

一口吃不了大胖子，我们从最简单也是最重要的算法谈起:

### 2PC(two-parse commit)

![image-20200328143029078](/Users/liuyl/Library/Application Support/typora-user-images/image-20200328143029078.png)

2pc最重要的就是TC(transaction coordinator)，它承担了客户和服务集群之间的稳定性与安全性的中间负载

#### 保证

+ 原子性

  必须满足AB同时提交或同时终止

+ 存活性

  一方出现通信中断，有能力终止访问并恢复正常服务

#### 何时提交

2pc规定:

1. 若向服务器发送`Yes`，则意味着它一定有能力完成这次响应，即使是故障重启后也能恢复该业务正常执行
2. 为保证`1`,在接收到TC发送的`prepare`之前，`cohort(参与者)`应该预执行这次事务，并保存`undo redo log`
3. 一旦TC发送`commit`，则`cobort`正式`commit`

#### 细节

##### 同步阻塞

在`TC`向`cohort(参与者)`发送`prepare`并等待表决的时间内，资源是被锁住的，这意味着如果网络通信中断，其他正常服务也将被阻塞

##### 预提交

2pc里，`TC`向`client`进行确认的时间并非是`cohort`执行成功的时候，而是接收到全部`cohort`的`commit`的时候。所以这必须要`cohort`有故障恢复的能力。

##### coordinator选举

为防止`coordinator`宕机，2PC拥有选举机制，此时可以新选一个节点成为`coordinator`

不过你需要注意的是，新的`coordinator`无法获得上个`coordinator`的信息，它无法处理全部的事情

##### 超时中断

为防止集群卡死，`coordinator`在时间范围内收不到某个`cohort`的`ack`时，它会直接`abort`掉整个事务。但你需要注意的是，2PC没有对`cohort`做超时中断，它只要没有接收到`coordinator`的响应时，就会锁死资源。

#### 存在问题

如果你仔细思考过2PC，应该不难发现，TC发送的`global commit`需要每个`voter`的配合，同时`voter`又需要`TC`发送的`global commit`才能执行下一步，所以关键节点是`[TC] + [voters]`

##### 关键节点宕机导致二义性问题

考虑下图的情况:

1. 在`commit`阶段，`coordinator`和`voter3`同时宕机，此时`voter1`和`voter2`已经commit
2. 安装2PC条件，必须要全部`voter`commit后，才会发送`global commit`指令
3. 此时`coordinator`宕机，其他`voter`根本不知道`voter3`目前的状况，出现以下歧义
   + `commit`后宕机(这种情况直接`commit`即可,因为`voter3`在故障恢复后自然会重新执行`commit`)
   + `voter3`发送`abort`,事务已经终结(这种情况所有的`cohort`应该`abort`)

![image-20200328151841344](/Users/liuyl/Library/Application Support/typora-user-images/image-20200328151841344.png)

#### 语义化

为便于理解，我们语义化2PC的行为

1. TC询问`cohort`是否<b>有能力</b>执行事务，此时`cohort`锁住资源
2. TC根据`cohorts`的响应，判断是否让`cohort`提交事务
3. `cohort`提交事务后，释放资源



#### 实例(mongodb的2pc)

在不使用`mongodb`的事务时，`mongodb`默认是只有单文档的原子操作的，此时需要用户层做`2pc`的提交。

按照上面的思路，我们引入中间的`coordinator`，一个独立的`transaction`集合，里面以文档的形式储存了事务。



#### 实例(mongodb的XA)

> XA是分布式二阶段提交事务

实际上，`mysql`的事务本身就有原子性，在单库事务里完全不需要用`XA`。但在分库分表的分布式事务里，需要使用`XA`来访问不同的库以完成事务。

在`innodb`里，`XA`依靠`binlog`和`redo`两个原子性的操作来实现。



### 3PC

![image-20200328150906196](/Users/liuyl/Library/Application Support/typora-user-images/image-20200328150906196.png)

> 3PC实现极为复杂，很少使用

3PC的存在是为了解决2PC的一些问题，包括同步阻塞etc...

#### 过程

##### Can-Commit

TC向`cohort`询问是否有意愿参与事务，若全部`cohort`都同意参与，则进入下一个阶段。

##### Pre-Commit

一旦接收到此信号，`cohort`就感知到其余全部`cohort`都有意愿参与事务，此时执行事务但不提交。

在这个阶段，`cohort`会维护一个超时计时器，若长时间无法收到TC的命令，直接`abort`事务。此时其他的`cohort`要么在`Do-Commit`阶段被TC发送`global-abort`，要么也因无法与TC通信而直接`abort`事务。保证数据一致性。

##### Do-Commit

若有`cohort`在`Pre-Commit`阶段执行失败或通信超时，将发送`global-abort`中断信号

#### 细节

##### 什么时候执行的事务提交

在流程图里我们可以看到，在`Pre-Commit`和`Do-Commit`两个阶段，`cohort`都执行了提交，为解释这两种提交的区别，我们介绍分布式最常见的概念:

###### 概念

+ redo

  redo是重做日志，可以利用`redo log`来做数据库复原

+ undo 

  undo是恢复日志，它记录了行为的反操作。

  > 可以这样认为，当 delete 一条记录时，undo log 中会记录一条对应的 insert 记录，当 update 一条记录时，它记录一条对应相反的 update 记录。

由此可知，`Pre-Commit`阶段其实已经执行了事务的提交，并且将`redo`和`undo`记录到事务日志

`Do-Commit`阶段根据`abort`或`commit`指令进行提交或重做(也就是说这是一个确认阶段)

#### 与2PC的差异

从流程图里可以看出，3PC把2PC的一阶段拆分为[Can-Commit,Pre-Commit]两个阶段

##### 如何解决2PC关键节点宕机问题

3PC引入了`Can-Commit`这个阶段，这让`voter`可以感知到以下两种情况

+ 其他`voter`都已经`commit` (此时会继续进入`Pre-Commit`阶段)
+ 某个`voter`进行了`abort`

因为在分布式事务中，不需要知道是哪个`voter abort`，只需要知道自身是否应该继续`commit`即可，所以一旦出现我们刚刚分析的2PC里关键节点同时宕机的事件时，`voter`就知道应该`commit`还是`abort`

##### 为何引入Pre-Commit

前文已经提到过，`Pre-Commit`阶段保证了`cohort`也拥有自己的超时定时器，在TC宕机或者网络出错的情况下，`cohort`可以直接中止事务。

<b>关键的是，为何在2PC里无法直接abort?</b>

原因很简单，在2PC中，如果`cohort`与TC通信失败就`abort`事务，那通信成功的`cohort`会提交事务，此时TC并没有办法再次联系`cohort`中止掉事务，导致一致性出现问题。

#### 致命缺陷

3PC无法满足P(分区容错性)。一旦网络出现分区，则会发生:

+ 部分`voter`收到`Pre-Commit`
+ 部分`voter`没有收到`Pre-Commit`,甚至无法与TC通信
+ 此时左右两部分`voters`将会做出不同行为，集群被划分。

#### 语义化

1. TC询问`cohort`是否<b>有意愿</b>执行事务

2. TC根据`cohorts`的响应选择是否让事务继续

   2.1 此时`cohort`了解其他`cohort`的行为 

3. TC询问`cohort`是否`有能力`执行事务

4. TC根据`cohort`响应选择是否提交事务

5. 提交事务后，`cohort`释放资源

> 你需要注意有意愿和有能力这两个关键词



### TCC(Try-Confirm-Cancel)

> TCC是分布式事务的一种实现，它有非常大的使用场景，几乎是分布式事务的代名词

TCC又称补偿事务。其核心思想是："针对每个操作都要注册一个与其对应的确认和补偿撤销操作"。它分为三个操作：

+ try 尝试
+ confirm 确认
+ cancel 取消

不再详细讲解，有需要可参考其他文章

美中不足的是，TCC对业务的侵入性非常大，实现难度较高。

### PAXOS



## 参考

[漫话分布式系统共识协议: 2PC/3PC篇](https://zhuanlan.zhihu.com/p/35298019)

